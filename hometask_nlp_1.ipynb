{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42637547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Установка нужных пакетов\n",
    "# !pip install --upgrade nltk gensim bokeh umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c9b51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "940364b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File './quora.txt' already there; not retrieving.\r\n"
     ]
    }
   ],
   "source": [
    "# выгружаем датасет:\n",
    "!wget \"https://www.dropbox.com/s/obaitrix9jyu84r/quora.txt?dl=1\" -O ./quora.txt -nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00541278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What TV shows or books help you read people's body language?\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = list(open(\"./quora.txt\", encoding=\"utf-8\"))\n",
    "data[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56391f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'TV', 'shows', 'or', 'books', 'help', 'you', 'read', 'people', \"'\", 's', 'body', 'language', '?']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "print(tokenizer.tokenize(data[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fe234",
   "metadata": {},
   "source": [
    "### Задание 1: Перевести все слова в нижний регистр (NLTK) из data и добавьте как лист токенов в листе data_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c6b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tok = []\n",
    "for i in range(len(data)): \n",
    "      data_tok.append(tokenizer.tokenize(data[i].lower()))\n",
    "  \n",
    "#checking\n",
    "assert all(isinstance(row, (list, tuple)) for row in data_tok), \"please convert each line into a list of tokens (strings)\"\n",
    "is_latin = lambda tok: all('a' <= x.lower() <= 'z' for x in tok)\n",
    "assert all(map(lambda l: not is_latin(l) or l.islower(), map(' '.join, data_tok))), \"please make sure to lowercase the data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18aea8c",
   "metadata": {},
   "source": [
    "### Задание 2: Подсчитайте топ10 самых популярных лем в рамках data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "411e77b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/olgakamskaa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/olgakamskaa/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>?</th>\n",
       "      <td>552413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>252068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>what</th>\n",
       "      <td>214798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>185392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>172513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>149873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>141788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>139813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>how</th>\n",
       "      <td>135687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>112001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "?     552413\n",
       "the   252068\n",
       "what  214798\n",
       "is    185392\n",
       "a     172513\n",
       "i     149873\n",
       "to    141788\n",
       "in    139813\n",
       "how   135687\n",
       "of    112001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "dict_words = {}\n",
    "\n",
    "for token in data_tok:\n",
    "      for i_word in token: \n",
    "        word_lemmatizing = lemmatizer.lemmatize(i_word)\n",
    "        if word_lemmatizing in dict_words.keys(): \n",
    "              dict_words[word_lemmatizing] += 1\n",
    "        else:\n",
    "              dict_words[word_lemmatizing] = 1\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_dict(dict_words, orient='index')\n",
    "df.sort_values(0, ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb7894",
   "metadata": {},
   "source": [
    "### Задание 3: Подсчитайте количество разных слов до и после лемматизации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5b2c937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество разных слов до лемматизации : 87819.000\n",
      "\n",
      "Количество разных слов после лемматизации : 80303.000\n"
     ]
    }
   ],
   "source": [
    "tok_words = {}\n",
    "for token in data_tok:\n",
    "      for word in token: \n",
    "            if word in tok_words.keys(): \n",
    "                  tok_words[word] += 1\n",
    "            else:\n",
    "                  tok_words[word] = 1\n",
    "    \n",
    "#raw word count\n",
    "word_size = len(tok_words.values())\n",
    "#size of lemmatized words\n",
    "lem_size = len(dict_words.values())\n",
    "print(\"Количество разных слов до лемматизации : %.3f\" % word_size)\n",
    "print(\"\\nКоличество разных слов после лемматизации : %.3f\" % lem_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c208b4b",
   "metadata": {},
   "source": [
    "### Задание 4: Подсчитайте количество разных слов до и после стемминга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf696c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество разных слов до стемминга : 87819.000\n",
      "\n",
      "Количество разных слов после стемминга : 67026.000\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stem = {}\n",
    "for token in data_tok:\n",
    "    for i_word in token: \n",
    "        stem_word = stemmer.stem(i_word)\n",
    "        if stem_word in stem.keys(): \n",
    "            stem[stem_word] += 1\n",
    "        else:\n",
    "            stem[stem_word] = 1\n",
    "\n",
    "stem_size = len(stem.values())\n",
    "\n",
    "print(\"Количество разных слов до стемминга : %.3f\" % word_size)\n",
    "print(\"\\nКоличество разных слов после стемминга : %.3f\" % stem_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e691822",
   "metadata": {},
   "source": [
    "### Задание 5: Подсчитайте количество разных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "375c1793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сначала лемматизация потом стемминг : 67026.000\n",
      "\n",
      "Сначала стемминг потом лемматизация: 80303.000\n"
     ]
    }
   ],
   "source": [
    "lemm_stemm = []\n",
    "\n",
    "for token in data_tok:\n",
    "    for i_word in token: \n",
    "        lemm_stemm.append(stemmer.stem(i_word))\n",
    "\n",
    "stemm_lemm = []\n",
    "\n",
    "for token in data_tok:\n",
    "    for i_word in token: \n",
    "        stemm_lemm.append(lemmatizer.lemmatize(i_word))\n",
    "        \n",
    "print(\"Сначала лемматизация потом стемминг : %.3f\" % len(set(lemm_stemm)))\n",
    "print(\"\\nСначала стемминг потом лемматизация: %.3f\" % len(set(stemm_lemm)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a40a55ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтобы не потерять слова сначала лучше делать лемматизацию"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a8a06",
   "metadata": {},
   "source": [
    "REGEXP\n",
    "\n",
    "https://www.programiz.com/python-programming/regex\n",
    "\n",
    "https://docs.python.org/3/howto/regex.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9008252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search successful.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "pattern = 'a[a-zA-Z]*s'\n",
    "test_string = 'abyss'\n",
    "result = re.match(pattern, test_string)\n",
    "\n",
    "if result:\n",
    "      print(\"Search successful.\")\n",
    "else:\n",
    "      print(\"Search unsuccessful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638d7e6",
   "metadata": {},
   "source": [
    "### Задание 6:\n",
    "https://www.hackerrank.com/challenges/matching-specific-string/problem?isFullScreen=true\n",
    "\n",
    "### Задание 7:\n",
    "https://www.hackerrank.com/challenges/matching-whitespace-non-whitespace-character/problem?isFullScreen=true\n",
    "\n",
    "### Задание 8:\n",
    "https://www.hackerrank.com/challenges/matching-start-end/problem?isFullScreen=true\n",
    "\n",
    "### Задание 9:\n",
    "https://www.hackerrank.com/challenges/matching-word-boundaries/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce41175a",
   "metadata": {},
   "source": [
    "Bag Of Words (BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "578ed602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
      "['learning', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n",
      "['welcome', 'great', 'learning', 'now', 'start', 'good', 'practice']\n",
      "[1, 1, 2, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "def vectorize(tokens):\n",
    "    ''' This function takes list of words in a sentence as input \n",
    "    and returns a vector of size of filtered_vocab.It puts 0 if the \n",
    "    word is not present in tokens and count of token if present.'''\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "def unique(sequence):\n",
    "    '''This functions returns a list in which the order remains \n",
    "    same and no item repeats.Using the set() function does not \n",
    "    preserve the original ordering,so i didnt use that instead'''\n",
    "    seen = set()\n",
    "    return [x for x in sequence if not (x in seen or seen.add(x))]\n",
    "\n",
    "#create a list of stopwords.You can import stopwords from nltk too\n",
    "stopwords=[\"to\",\"is\",\"a\"]\n",
    "\n",
    "#list of special characters.You can use regular expressions too\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"]\n",
    "\n",
    "#Write the sentences in the corpus,in our case, just two \n",
    "string1=\"Welcome to Great Learning , Now start learning\"\n",
    "string2=\"Learning is a good practice\"\n",
    "\n",
    "#convert them to lower case\n",
    "string1=string1.lower()\n",
    "string2=string2.lower()\n",
    "\n",
    "#split the sentences into tokens\n",
    "tokens1=string1.split()\n",
    "tokens2=string2.split()\n",
    "print(tokens1)\n",
    "print(tokens2)\n",
    "\n",
    "#create a vocabulary list\n",
    "vocab=unique(tokens1+tokens2)\n",
    "print(vocab)\n",
    "\n",
    "#filter the vocabulary list\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "print(filtered_vocab)\n",
    "\n",
    "#convert sentences into vectords\n",
    "vector1=vectorize(tokens1)\n",
    "print(vector1)\n",
    "vector2=vectorize(tokens2)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb5ca69",
   "metadata": {},
   "source": [
    "### Задание 10: Реализовать Bag of words на data_tok (можно на NLTK, можно без)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a8d15e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "first50 = ''\n",
    "for line in data[0:50]:\n",
    "      first50 += line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7e53c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'they', 'below', 'so', 'couldn', 'few', 'over', \"mightn't\", 'is', \"needn't\", \"won't\", 'hasn', 'he', \"it's\", 'above', \"haven't\", 'on', 'because', 'll', 'wasn', 'here', 've', 'itself', 'won', 'than', 'then', 'such', 'to', 'just', 'shan', \"doesn't\", 'why', 'yours', 'has', 'o', 'of', 'can', 'ourselves', 'herself', 'a', 'about', 'him', \"wouldn't\", 'from', 'if', 'each', 'this', 'where', 'y', \"hadn't\", 're', \"you'll\", 'for', 'their', \"you're\", 'all', 'but', 'themselves', 'be', 'by', \"you've\", 'until', 'having', 'now', 'through', 'very', 'mightn', 'same', 'ain', 'were', 'you', 'how', \"you'd\", 't', 'not', 'ma', 'these', 'which', 'those', 'doing', 'an', \"weren't\", 'me', 'further', 'your', \"isn't\", 'against', 'our', 'do', 'before', 'am', 'will', 'aren', 'after', 'shouldn', 'during', 'out', 'himself', 'her', \"that'll\", 'that', 'into', 'she', 'didn', 'isn', 'with', 'down', 'yourself', 'mustn', 'we', 'wouldn', \"didn't\", 'there', 'should', 'whom', 'hers', 'been', 'too', 'yourselves', 'weren', 'again', 'as', 'are', 'and', 'under', 'haven', \"mustn't\", 'up', 'what', 'once', 'other', 'ours', 'most', 'myself', 'more', 'some', 'nor', 'did', 'at', 'no', \"don't\", \"aren't\", 'was', 'both', 'them', \"couldn't\", 'only', 'm', 'needn', 'its', 'between', 'it', 's', 'own', \"shan't\", 'theirs', 'in', \"shouldn't\", \"hasn't\", 'i', 'or', 'does', 'hadn', 'any', 'd', \"wasn't\", 'while', 'his', 'who', 'my', \"she's\", 'being', 'had', 'have', 'when', 'the', 'doesn', 'off', \"should've\", 'don'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55027c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TextPreproc (text: str, uniqs: bool=True):\n",
    "    '''This function processes the incoming text: \n",
    "    tokenizes, removes punctuation and stop words, lemmatizes. \n",
    "    Returns a list of processed гтшйгуwords.'''\n",
    "    text = text.lower()\n",
    "    tokens_text = tokenizer.tokenize(text)\n",
    "    words = [word for word in tokens_text if word.isalpha()]\n",
    "    words = [w for w in words if not w in str(stops)]\n",
    "    func_lem = lambda w: lemmatizer.lemmatize(w)\n",
    "    lem_words = list(map(func_lem, words))\n",
    "\n",
    "    return (set(lem_words) if uniqs else lem_words) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "937ecab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TextPreproc(first50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b50602c",
   "metadata": {},
   "source": [
    "### Дополнительное задание. Найти наиболее близкие вектора/строки среди первых 50 строк quora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a7aea69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = TextPreproc(first50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38871b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5773502691896258\n",
      "3 What are ZIP codes in the Bay Area?\n",
      "\n",
      "11 What is the ZIP code of India?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matrix = []\n",
    "\n",
    "for line in data[0:50]:\n",
    "    prepoc_line = TextPreproc(line, False)\n",
    "    temp_vector = []\n",
    "    for i in vector:\n",
    "        if i in prepoc_line:\n",
    "              temp_vector.append(1)\n",
    "        else:\n",
    "              temp_vector.append(0)\n",
    "    matrix.append(temp_vector)\n",
    "\n",
    "    \n",
    "cos = [0, 0, 0]\n",
    "for i in range(len(matrix)-1):\n",
    "      for j in range(i+1, len(matrix)):\n",
    "        a = matrix[i]\n",
    "        b = matrix[j]\n",
    "        cos_sim = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "        if cos_sim > cos[0]:\n",
    "            cos = [cos_sim, i, j]\n",
    "print(cos[0])\n",
    "print(cos[1], data[cos[1]])\n",
    "print(cos[2], data[cos[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7213d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c85e018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
